{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from torchvision.transforms import transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11d2bd5d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_config = [\n",
    "    #Tuple: (kernel_size, number_of_filters, strides, padding)\n",
    "    # \"M\": Max Pool Layer\n",
    "    (7, 64, 2, 3),\n",
    "    \"M\",\n",
    "    (3, 192, 1, 1),\n",
    "    \"M\",\n",
    "    (1, 128, 1, 1),\n",
    "    (3, 256, 1, 1),\n",
    "    (1, 256, 1, 0),\n",
    "    (3, 512, 1, 1),\n",
    "    \"M\",\n",
    "    #List: [(tuple), (tuple), how many times to repeat]\n",
    "    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n",
    "    (1, 512, 1, 0),\n",
    "    (3, 1024, 1, 1),\n",
    "    \"M\",\n",
    "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 2, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "    #Doesn't include FC layers\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.leakyrelu(self.batchnorm(self.conv(x)))\n",
    "\n",
    "class YoloV1(nn.Module):\n",
    "    def __init__(self, in_channels=3, **kwargs):\n",
    "        super(YoloV1, self).__init__()\n",
    "        self.architecture = architecture_config\n",
    "        self.in_channels = in_channels\n",
    "        self.darknet = self._create_conv_layers(self.architecture)\n",
    "        self.fcs = self._create_fcs(**kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.darknet(x)\n",
    "        return self.fcs(torch.flatten(x, start_dim=1))\n",
    "\n",
    "    def _create_conv_layers(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for x in architecture:\n",
    "            if type(x) == tuple:\n",
    "                layers+= [CNNBlock(in_channels, x[1], kernel_size=x[0], stride=x[2], padding=x[3])]\n",
    "                in_channels = x[1]\n",
    "            elif type(x) == str:\n",
    "                layers+= [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            elif type(x) == list:\n",
    "                conv1 = x[0] #tuple\n",
    "                conv2 = x[1] #tuple\n",
    "                repeats = x[2] #int\n",
    "\n",
    "                for _ in range(repeats):\n",
    "                    layers+=[CNNBlock(in_channels, conv1[1], kernel_size=conv1[0], stride=conv1[2], padding=conv1[3])]\n",
    "                    layers+=[CNNBlock(conv1[1], conv2[1], kernel_size=conv2[0], stride=conv2[2], padding=conv2[3])]\n",
    "                    in_channels=conv2[1]\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _create_fcs(self, split_size, num_boxes, num_classes):\n",
    "        S, B, C = split_size, num_boxes, num_classes\n",
    "        return nn.Sequential(nn.Flatten(), nn.Linear(1024*S*S, 496), nn.Dropout(0.0), nn.LeakyReLU(0.1), nn.Linear(496, S*S*(C+B*5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersection over Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(boxes_preds, boxes_labels, box_format = 'midpoint'):\n",
    "    \"\"\"\n",
    "    Calculates Intersection over union\n",
    "\n",
    "    Parameters:\n",
    "        boxes_preds (tensor): Prediction of Bounding boxes(BATCH_SIZE, 4)\n",
    "        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        box_format (str): midpoint/corners, if boxes are (x,y,w,h) or (x1,y1,x2,y2) respectively\n",
    "\n",
    "    Returns:\n",
    "        tensor: Intersection over union for all examples\n",
    "    \"\"\"\n",
    "    # boxes_preds shape is (N,4) where N is the number of predicted bboxes\n",
    "    # boxes_labels shape is (n, 4)\n",
    "\n",
    "    if box_format == \"midpoint\":\n",
    "        box1_x1 = boxes_preds[...,0:1] - boxes_preds[...,2:3] / 2\n",
    "        box1_y1 = boxes_preds[...,1:2] - boxes_preds[...,3:4] / 2\n",
    "        box1_x2 = boxes_preds[...,0:1] + boxes_preds[...,2:3] / 2\n",
    "        box1_y2 = boxes_preds[...,1:2] + boxes_preds[...,3:4] / 2\n",
    "\n",
    "        box2_x1 = boxes_labels[...,0:1] - boxes_labels[...,2:3] / 2\n",
    "        box2_y1 = boxes_labels[...,1:2] - boxes_labels[...,3:4] / 2\n",
    "        box2_x2 = boxes_labels[...,0:1] + boxes_labels[...,2:3] / 2\n",
    "        box2_y2 = boxes_labels[...,1:2] + boxes_labels[...,3:4] / 2\n",
    "\n",
    "    if box_format == \"corners\":\n",
    "        box1_x1 = boxes_preds[...,0:1]\n",
    "        box1_y1 = boxes_preds[...,1:2]\n",
    "        box1_x2 = boxes_preds[...,2:3]\n",
    "        box1_y2 = boxes_preds[...,3:4] #output tensor should (N,1). If we only use 3, we go to (N)\n",
    "\n",
    "        box2_x1 = boxes_labels[...,0:1]\n",
    "        box2_y1 = boxes_labels[...,1:2]\n",
    "        box2_x2 = boxes_labels[...,2:3]\n",
    "        box2_y2 = boxes_labels[...,3:4]\n",
    "\n",
    "    \n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.max(box1_x2, box2_x2)\n",
    "    y2 = torch.max(box1_y2, box2_y2)\n",
    "\n",
    "    # .clamp(0) is for the case when they don't intersect. Since when they don't intersect one of these will be negetive so they should become 0\n",
    "    intersection = (x2 - x1).clamp(0) * (y2-y1).clamp(0)\n",
    "\n",
    "    box1_area = abs((box1_x2- box1_x1)*(box1_y2-box1_y1))\n",
    "    box2_area = abs((box2_x2- box2_x1)*(box2_y2-box2_y1))\n",
    "\n",
    "    return intersection / (box1_area + box2_area - intersection + 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Maximal Supression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_supression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n",
    "    \"\"\"\n",
    "    Given Bounding Boxes does Non-maximal supression\n",
    "\n",
    "    Parameters:\n",
    "        bboxes (list): list of lists containing all bounding boxes with each box specified as [class_preds, prob_score, x1, y2, x2, y2]\n",
    "        iou_threshold: (float): threshold where predicted bounding box is correct\n",
    "        threshold (float): threshold to remove predicted bboxes (Independent of IOU)\n",
    "        box_format (str): midpoint/corners, if boxes are (x,y,w,h) or (x1,y1,x2,y2) respectively\n",
    "    \n",
    "    Returns:\n",
    "        bboxes_after_nms (list): bboxes after performing NMS given a specific IOU Threshold\n",
    "    \"\"\"\n",
    "\n",
    "    assert type(bboxes) == list\n",
    "\n",
    "    bboxes = [box for box in bboxes if box[1]> threshold ]\n",
    "    bboxes = sorted(bboxes, key= lambda x: x[1], reverse=True)\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    while bboxes:\n",
    "        chosen_box = bboxes.pop(0)\n",
    "        bboxes = [box for box in bboxes \n",
    "        if box[0]!=chosen_box[0] or intersection_over_union(torch.tensor(chosen_box[2:]), torch.tensor(box[2:]), box_format = box_format) < iou_threshold\n",
    "        ]\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "    \n",
    "    return bboxes_after_nms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Average Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20):\n",
    "    \"\"\"\n",
    "    Calculates Mean Average Precision\n",
    "\n",
    "    Parameters:\n",
    "        pred_boxes (list): list of lists containing all bounding boxes with each box specified as [train_idx, class_preds, prob_score, x1, y2, x2, y2]\n",
    "        true_boxes (list): similar to pred boxes except all the correct ones\n",
    "        iou_threshold (float): threshold above which predicted box is correct\n",
    "        box_format (str): midpoint/corners, if boxes are (x,y,w,h) or (x1,y1,x2,y2) respectively\n",
    "        num_classes (int): number of classes\n",
    "\n",
    "    Returns:\n",
    "        float: mAP value across all classes given a specific IOU threshold\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #list storing all AP for respective classes\n",
    "    average_precisions = []\n",
    "\n",
    "    #used for numerical stability later on\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        # Go through all the predictions and targets and only add the ones that belong to the current class c\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1]==c:\n",
    "                detections.append(detection)\n",
    "\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1]==c:\n",
    "                ground_truths.append(true_box)\n",
    "\n",
    "        # find the amount of bboxes for each training example, Counter here finds how many ground truth boxes we get for each training example.\n",
    "        # so lets say img 0 has 3 and img 1 has 5 then we will get dictionary with amount_bboxes={0:3, 1:5}\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "\n",
    "        # We then go through each key, val in this dictionary and convert to the following (w.r.t. same example)\n",
    "        #  amount_bboxes = {0: torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        # sort by box_probabilities which is index 2\n",
    "        detections.sort(key= lambda x:x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "\n",
    "        # if none exists for this class then we can safely skip\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            # only take out ground truths that have the same training_idx as detection\n",
    "            ground_truth_img = [bbox for bbox in ground_truths if bbox[0] == detection[0]]\n",
    "\n",
    "            num_gts = len(ground_truth_img)\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(torch.tensor(detection[3:]), torch.tensor(gt[3:]), box_format= box_format)\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "            \n",
    "            if best_iou > iou_threshold:\n",
    "                # only detect ground truth detection once\n",
    "                if amount_bboxes[detection[0]][best_gt_idx]==0:\n",
    "                    #true positive and add this bounding box to seen\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "            # if iou is lower then the detection is false positive\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "        \n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        # torch.trapz for numerical integration\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "    return sum(average_precisions)/ len(average_precisions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes(loader, model, iou_threshold, threshold, pred_format = \"cells\", box_format = \"midpoint\", device= \"cuda\"):\n",
    "    \n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "\n",
    "    # make sure model is in eval() mode before get bboxes\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "\n",
    "    for batch_idx, (x, labels) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        true_bboxes = cellboxes_to_boxes(labels)\n",
    "        bboxes = cellboxes_to_boxes(predictions)\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            nms_boxes = non_max_supression(bboxes[idx],iou_threshold= iou_threshold, threshold = threshold, box_format = box_format)\n",
    "\n",
    "            for nms_box in nms_boxes:\n",
    "                all_pred_boxes.append([train_idx]+ nms_box)\n",
    "\n",
    "            for box in true_bboxes[idx]:\n",
    "                #how many will converted to 0 pred\n",
    "                if box[1]>threshold:\n",
    "                    all_true_boxes.append([train_idx]+box)\n",
    "\n",
    "            train_idx+=1\n",
    "            \n",
    "    model.train()\n",
    "    return all_pred_boxes, all_true_boxes     \n",
    "\n",
    "def convert_cellboxes(predictions, S=7, C=3):\n",
    "    \"\"\"\n",
    "    Converts bounding boxes output from Yolo with an image split size of S into entire image ratios rather than relative to cell ratios.\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = predictions.to(\"cpu\")\n",
    "    batch_size = predictions.shape[0]\n",
    "    predictions = predictions.reshape(batch_size, 7, 7, C + 10)\n",
    "    bboxes1 = predictions[..., C+1:C+5]\n",
    "    bboxes2 = predictions[..., C+6: C+10]\n",
    "    scores = torch.cat((predictions[...,C].unsqueeze(0), predictions[...,C+5].unsqueeze(0)), dim=0)\n",
    "    best_box = scores.argmax(0).unsqueeze(-1)\n",
    "    best_boxes = bboxes1 * (1-best_box)+ best_box * bboxes2\n",
    "    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n",
    "    x = 1 / S * (best_boxes[..., :1]+ cell_indices)\n",
    "    y = 1 / S * (best_boxes[..., 1:2]+ cell_indices.permute(0,2,1,3))\n",
    "    w_y = 1 / S * best_boxes[..., 2:4]\n",
    "    converted_bboxes = torch.cat((x,y,w_y), dim=-1)\n",
    "    predicted_class = predictions[...,:C].argmax(-1).unsqueeze(-1)\n",
    "    best_confidence = torch.max(predictions[..., C], predictions[...,C+5]).unsqueeze(-1)\n",
    "    converted_preds = torch.cat((predicted_class, best_confidence, converted_bboxes), dim=-1)\n",
    "\n",
    "    return converted_preds\n",
    "\n",
    "def cellboxes_to_boxes(out, S=7):\n",
    "    converted_preds = convert_cellboxes(out).reshape(out.shape[0], S*S, -1)\n",
    "    converted_preds[...,0] = converted_preds[..., 0].long()\n",
    "    all_bboxes = []\n",
    "\n",
    "    for ex_idx in range(out.shape[0]):\n",
    "        bboxes = []\n",
    "        \n",
    "        for bbox_idx in range(S*S):\n",
    "            bboxes.append([x.item() for x in converted_preds[ex_idx, bbox_idx, :]])\n",
    "        all_bboxes.append(bboxes)\n",
    "    \n",
    "    return all_bboxes\n",
    "\n",
    "def save_checkpoint(state, filename = \"my_checkpoint.pth\"):\n",
    "    print(\"=> Saving Checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading Checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dir = 'dataset/train_zip/train'\n",
    "test_dir = 'dataset/test_zip/test'\n",
    "\n",
    "images = [image for image in sorted(os.listdir(files_dir)) if image[-4:]=='.jpg']\n",
    "annots = [image[:-4] + '.xml' for image in images]\n",
    "\n",
    "images = pd.Series(images, name='images')\n",
    "annots = pd.Series(annots, name='annots')\n",
    "df = pd.concat([images, annots], axis=1)\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "test_images = [image for image in sorted(os.listdir(test_dir)) if image[-4:]=='.jpg']\n",
    "test_annots = [image[:-4] + '.xml' for image in test_images]\n",
    "\n",
    "test_images = pd.Series(test_images, name='test_images')\n",
    "test_annots = pd.Series(test_annots, name='test_annots')\n",
    "test_df = pd.concat([test_images, test_annots], axis=1)\n",
    "test_df = pd.DataFrame(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FruitImagesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df=df, files_dir=files_dir, S=7, B=2, C=3, transform =None):\n",
    "        self.annotations = df\n",
    "        self.files_dir = files_dir\n",
    "        self.transform = transform\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label_path = os.path.join(self.files_dir, self.annotations.iloc[index, 1])\n",
    "        boxes = []\n",
    "        tree = ET.parse(label_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        class_dictionary = {'apple': 0, 'banana': 1, 'orange':2}\n",
    "\n",
    "        if(int(root.find('size').find('height').text)==0):\n",
    "            filename = root.find('filename').text\n",
    "            img = Image.open(self.files_dir + '/' + filename)\n",
    "            img_width, img_height = img.size\n",
    "\n",
    "            for member in root.findall('object'):\n",
    "                klass = member.find('name').text\n",
    "                klass = class_dictionary[klass]\n",
    "\n",
    "                #bounding box\n",
    "                xmin = int(member.find('bndbox').find('xmin').text)\n",
    "                xmax = int(member.find('bndbox').find('xmax').text)\n",
    "                ymin = int(member.find('bndbox').find('ymin').text)\n",
    "                ymax = int(member.find('bndbox').find('ymax').text)\n",
    "\n",
    "                centerx = ((xmax + xmin)/2) /img_width\n",
    "                centery = ((ymax+ ymin)/2) / img_height\n",
    "                boxwidth = (xmax - xmin) / img_width\n",
    "                boxheight = (ymax - ymin) / img_height\n",
    "\n",
    "                boxes.append([klass, centerx, centery, boxwidth, boxheight])\n",
    "\n",
    "        elif(int(root.find('size').find('height').text)!=0):\n",
    "            \n",
    "            for member in root.findall('object'):\n",
    "                klass = member.find('name').text\n",
    "                klass = class_dictionary[klass]\n",
    "\n",
    "                #bounding box\n",
    "                xmin = int(member.find('bndbox').find('xmin').text)\n",
    "                xmax = int(member.find('bndbox').find('xmax').text)\n",
    "                img_width = int(root.find('size').find('width').text)\n",
    "                ymin = int(member.find('bndbox').find('ymin').text)\n",
    "                ymax = int(member.find('bndbox').find('ymax').text)\n",
    "                img_height = int(root.find('size').find('height').text)\n",
    "\n",
    "                centerx = ((xmax + xmin)/2) /img_width\n",
    "                centery = ((ymax+ ymin)/2) / img_height\n",
    "                boxwidth = (xmax - xmin) / img_width\n",
    "                boxheight = (ymax - ymin) / img_height\n",
    "\n",
    "                boxes.append([klass, centerx, centery, boxwidth, boxheight])\n",
    "\n",
    "        boxes = torch.tensor(boxes)\n",
    "        img_path = os.path.join(self.files_dir, self.annotations.iloc[index, 0])\n",
    "        image = Image.open(img_path)\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image, boxes = self.transform(image, boxes)\n",
    "\n",
    "        #convert to cells\n",
    "        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
    "        for box in boxes:\n",
    "            class_label, x, y, width, height = box.tolist()\n",
    "            class_label = int(class_label)\n",
    "\n",
    "            #i,j represents the cell_row and cell_column\n",
    "            i,j = int(self.S*y), int(self.S*x)\n",
    "            x_cell, y_cell = self.S*x-j, self.S*y-i\n",
    "\n",
    "            \"\"\"\n",
    "            Calculating the width and height of cell of bounding box, relative to the cell is done by following, with width as the example:\n",
    "            \n",
    "            width_pixels = (width*self.image_width)\n",
    "            cell_pixels = (self.image_width)\n",
    "\n",
    "            Then to find the width relative to the cell is simply:\n",
    "            width_pixels/ cell_pixels, simplification leads to the formulas below.\n",
    "            \"\"\"\n",
    "\n",
    "            width_cell, height_cell = (width * self.S, height * self.S)\n",
    "\n",
    "            #If no object already found for specific cell i, j Note: this means we restrict to ONE object per cell !\n",
    "\n",
    "            if label_matrix[i, j, self.C] == 0:\n",
    "                #set that there exists an object\n",
    "                label_matrix[i, j , self.C] = 1\n",
    "                # box coordinates\n",
    "                box_coordinates = torch.tensor([x_cell, y_cell, width_cell, height_cell])\n",
    "                label_matrix[i,j, class_label] = 1\n",
    "\n",
    "        return image, label_matrix            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate the loss for YoloV1 model\n",
    "    \"\"\"\n",
    "    def __init__(self, S=7, B=2, C=3):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        \"\"\"\n",
    "        S is split size of image (grid size) (in paper 7)\n",
    "        B is number of boxes (in paper 2),\n",
    "        C is number of classes (in paper 20, in dataset 3)\n",
    "        \"\"\"\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        # input prediction shape: (BATCH_SIZE, S*S*(C+B*5))\n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n",
    "\n",
    "        #Calculate IOU for two predicted bounding boxes with target box\n",
    "        iou_b1 = intersection_over_union(predictions[...,self.C + 1: self.C+5], target[..., self.C + 1: self.C + 5])\n",
    "        iou_b2 = intersection_over_union(predictions[...,self.C + 6: self.C+10], target[..., self.C + 1: self.C + 5])\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "\n",
    "        #Take the box with highest IOU out of the two predictions. Note: bestbox will be indices 0,1 for which bbox was best\n",
    "        iou_maxes, bestbox = torch.max(ious, dim=0)\n",
    "        exists_box = target[..., self.C].unsqueeze(3)\n",
    "\n",
    "        # =========================#\n",
    "        # FOR BOX COORDINATES      #\n",
    "        # =========================#\n",
    "\n",
    "        #set boxes with no object in them to 0. We only take out one of the two predictions, which is one with highest IOUs\n",
    "        box_predictions = exists_box * ((bestbox * predictions[..., self.C + 6: self.C+10]+ (1- bestbox)* predictions[..., self.C + 1: self.C + 5]))\n",
    "        box_targets = exists_box * target[..., self.C + 1: self.C+ 5]\n",
    "\n",
    "        # take sqrt of width and height of box\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4])* torch.sqrt(torch.abs(box_predictions[...,2:4]+ 1e-6))\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "\n",
    "        box_loss = self.mse(torch.flatten(box_predictions, end_dim=-2),\n",
    "                            torch.flatten(box_targets, end_dim=-2))\n",
    "\n",
    "        # =========================#\n",
    "        # FOR  OBJECT LOSS      #\n",
    "        # =========================#\n",
    "\n",
    "        #pred_box is the confidence score for bbox with highest IOU\n",
    "        pred_box = (bestbox * predictions[..., self.C + 5:self.C+6] + (1-bestbox)* predictions[..., self.C:self.C+1])\n",
    "        object_loss = self.mse(torch.flatten(exists_box* pred_box),\n",
    "                                torch.flatten(exists_box * target[..., self.C: self.C+1]))\n",
    "\n",
    "\n",
    "        # =========================#\n",
    "        # FOR  No OBJECT LOSS      #\n",
    "        # =========================#\n",
    "\n",
    "        no_object_loss = self.mse(torch.flatten((1-exists_box)* predictions[..., self.C:self.C+1], start_dim=1),\n",
    "                                torch.flatten((1-exists_box)* target[..., self.C:self.C+1], start_dim=1))\n",
    "\n",
    "        no_object_loss += self.mse(torch.flatten((1-exists_box)* predictions[..., self.C+5:self.C+6], start_dim=1),\n",
    "                                torch.flatten((1-exists_box)* target[..., self.C:self.C+1], start_dim=1))\n",
    "\n",
    "        # =========================#\n",
    "        # FOR  CLASS LOSS      #\n",
    "        # =========================#\n",
    "\n",
    "        class_loss = self.mse(torch.flatten(exists_box * predictions[..., :self.C], end_dim=-2),\n",
    "                            torch.flatten(exists_box * target[..., :self.C], end_dim=-2))\n",
    "\n",
    "        loss = (self.lambda_coord * box_loss + object_loss + self.lambda_noobj * no_object_loss + class_loss)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 20\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "LOAD_MODEL_FILE = \"model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, optimizer, loss_fn):\n",
    "\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    mean_loss = []\n",
    "\n",
    "    for batch_idx, (x,y) in enumerate(loop):\n",
    "        x,y = x.to(DEVICE), y.to(DEVICE)\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "        mean_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_postfix(loss= loss.item())\n",
    "    \n",
    "    print(f\"Mean loss was: {sum(mean_loss)/ len(mean_loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "    def __init__(self,transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, bboxes):\n",
    "        for t in self.transforms:\n",
    "            img, bboxes = t(img), bboxes\n",
    "\n",
    "        return img, bboxes\n",
    "\n",
    "transform = Compose([transforms.Resize((448, 448)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/YoloV1/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      " 20%|██        | 3/15 [00:33<02:14, 11.17s/it, loss=171]/opt/homebrew/Caskroom/miniconda/base/envs/YoloV1/lib/python3.8/site-packages/PIL/Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [03:06<00:00, 12.46s/it, loss=107] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was: 107.46256052652994\n",
      "Train mAP: 0.0\n",
      "=> Saving Checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [02:54<00:00, 11.61s/it, loss=107] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss was: 82.15511932373047\n",
      "Train mAP: 0.0\n",
      "=> Saving Checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:27<06:20, 27.15s/it, loss=88.5]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m         checkpoint \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict()}\n\u001b[1;32m     26\u001b[0m         save_checkpoint(checkpoint, filename\u001b[38;5;241m=\u001b[39mLOAD_MODEL_FILE)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[75], line 18\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset \u001b[38;5;241m=\u001b[39m test_dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     pred_boxes, target_boxes \u001b[38;5;241m=\u001b[39m get_bboxes(train_loader, model, iou_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m, device\u001b[38;5;241m=\u001b[39mDEVICE)\n\u001b[1;32m     20\u001b[0m     mean_avg_prec \u001b[38;5;241m=\u001b[39m mean_average_precision(pred_boxes, target_boxes, iou_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, box_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmidpoint\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[59], line 12\u001b[0m, in \u001b[0;36mtrain_fn\u001b[0;34m(train_loader, model, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m     10\u001b[0m mean_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     15\u001b[0m loop\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/YoloV1/lib/python3.8/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/YoloV1/lib/python3.8/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/YoloV1/lib/python3.8/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    model = YoloV1(split_size= 7, num_boxes= 2, num_classes= 3).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer = optimizer, factor = 0.1, patience = 3, mode='max', verbose=True)\n",
    "    loss_fn = YoloLoss()\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optmizer)\n",
    "\n",
    "    train_dataset = FruitImagesDataset(transform = transform,files_dir = files_dir)\n",
    "    test_dataset = FruitImagesDataset(transform = transform,files_dir = test_dir)\n",
    "\n",
    "    train_loader = DataLoader(dataset = train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "    test_loader = DataLoader(dataset = test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_fn(train_loader, model, optimizer, loss_fn)\n",
    "        pred_boxes, target_boxes = get_bboxes(train_loader, model, iou_threshold=0.5, threshold=0.4, device=DEVICE)\n",
    "        mean_avg_prec = mean_average_precision(pred_boxes, target_boxes, iou_threshold=0.5, box_format = 'midpoint')\n",
    "\n",
    "        print(f\"Train mAP: {mean_avg_prec}\")\n",
    "        scheduler.step(mean_avg_prec)\n",
    "\n",
    "        checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "        save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE)\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_MODEL = True\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions():\n",
    "\n",
    "    model = YoloV1(split_size = 7, num_boxes= 2, num_classes=3).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "    loss_fn = YoloLoss()\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optmizer)\n",
    "\n",
    "    test_dataset = FruitImagesDataset(transform = transform,files_dir = test_dir)\n",
    "    test_loader = DataLoader(dataset = test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.eval()\n",
    "        train_fn(test_loader, model, optimizer, loss_fn)\n",
    "        pred_boxes, target_boxes = get_bboxes(test_loader, model, iou_threshold=0.5, threshold=0.4)\n",
    "        mean_avg_prec = mean_average_precision(pred_boxes, target_boxes, iou_threshold= 0.5, box_format=\"midpoint\")\n",
    "\n",
    "        print(f\"Test mAP: {mean_avg_prec}\")\n",
    "\n",
    "predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YoloV1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
