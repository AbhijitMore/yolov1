{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26161,"status":"ok","timestamp":1728708188960,"user":{"displayName":"Abhijit More","userId":"12603566416311184813"},"user_tz":-330},"id":"TDz7YOvV-lKf","outputId":"8351db00-2081-42b8-9872-219da0ce53b5"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"8YyeoM2Z-iuw"},"source":["# Import Libraries"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":7641,"status":"ok","timestamp":1728708199959,"user":{"displayName":"Abhijit More","userId":"12603566416311184813"},"user_tz":-330},"id":"Vp9TvoXF-iu0"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import pandas as pd\n","import os\n","from torchvision.transforms import transforms\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","import yaml\n","import mlflow\n","\n","from dataset import FruitImagesDataset\n","from utils import YoloLoss, intersection_over_union, non_max_supression, mean_average_precision, get_bboxes, save_checkpoint, load_checkpoint\n","from nets import YoloV1"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1728708199959,"user":{"displayName":"Abhijit More","userId":"12603566416311184813"},"user_tz":-330},"id":"PwSCbURu-iu1","outputId":"489278df-4df8-416f-84b3-c2df957507c5"},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x116e911f0>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["seed = 42\n","torch.manual_seed(seed)"]},{"cell_type":"markdown","metadata":{"id":"vQ0OPX9O-iu6"},"source":["# Dataset Preprocessing"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1728708480038,"user":{"displayName":"Abhijit More","userId":"12603566416311184813"},"user_tz":-330},"id":"mv_OgfqR-iu6"},"outputs":[],"source":["files_dir = 'dataset/train_zip/train'\n","test_dir = 'dataset/test_zip/test'\n","\n","images = [image for image in sorted(os.listdir(files_dir)) if image[-4:]=='.jpg']\n","annots = [image[:-4] + '.xml' for image in images]\n","\n","images = pd.Series(images, name='images')\n","annots = pd.Series(annots, name='annots')\n","df = pd.concat([images, annots], axis=1)\n","df = pd.DataFrame(df)\n","\n","test_images = [image for image in sorted(os.listdir(test_dir)) if image[-4:]=='.jpg']\n","test_annots = [image[:-4] + '.xml' for image in test_images]\n","\n","test_images = pd.Series(test_images, name='test_images')\n","test_annots = pd.Series(test_annots, name='test_annots')\n","test_df = pd.concat([test_images, test_annots], axis=1)\n","test_df = pd.DataFrame(test_df)"]},{"cell_type":"markdown","metadata":{"id":"-wVqK_Ye-iu8"},"source":["# Model Training"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":515,"status":"ok","timestamp":1728712528349,"user":{"displayName":"Abhijit More","userId":"12603566416311184813"},"user_tz":-330},"id":"WaG4QFH4-iu9"},"outputs":[],"source":["# Constants\n","LEARNING_RATE = 2e-5\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","BATCH_SIZE = 16\n","WEIGHT_DECAY = 0\n","EPOCHS = 20\n","NUM_WORKERS = 2\n","PIN_MEMORY = True\n","LOAD_MODEL = False\n","LOAD_MODEL_FILE = \"model.pth\""]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Training function with interactive progress bar\n","def train_one_epoch(train_loader, model, optimizer, loss_fn, epoch, total_epochs, scheduler=None):\n","    model.train()\n","    loop = tqdm(train_loader, leave=True)\n","    total_loss = 0\n","\n","    for batch_idx, (x, y) in enumerate(loop):\n","        x, y = x.to(DEVICE), y.to(DEVICE)\n","\n","        # Forward pass\n","        predictions = model(x)\n","        loss = loss_fn(predictions, y)\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","        # Structured progress bar updates\n","        loop.set_description(f\"Epoch [{epoch + 1}/{total_epochs}]\")\n","        loop.set_postfix({\n","            'Batch': f\"{batch_idx + 1}/{len(train_loader)}\",\n","            'Loss': f\"{loss.item():.4f}\",\n","            'Mean Loss': f\"{total_loss / (batch_idx + 1):.4f}\",\n","            'LR': optimizer.param_groups[0]['lr']\n","        })\n","\n","    mean_loss = total_loss / len(train_loader)\n","    print(f\"Mean loss for epoch {epoch + 1}: {mean_loss:.4f}\")\n","    return mean_loss"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Evaluation function to calculate mAP (Mean Average Precision)\n","def evaluate_model(loader, model, iou_threshold=0.5, threshold=0.4):\n","    model.eval()\n","    pred_boxes, target_boxes = get_bboxes(loader, model, iou_threshold=iou_threshold, threshold=threshold, device=DEVICE)\n","    mean_avg_prec = mean_average_precision(pred_boxes, target_boxes, iou_threshold=iou_threshold, box_format=\"midpoint\", num_classes=3)\n","    return mean_avg_prec"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["class Compose(object):\n","    def __init__(self,transforms):\n","        self.transforms = transforms\n","\n","    def __call__(self, img, bboxes):\n","        for t in self.transforms:\n","            img, bboxes = t(img), bboxes\n","\n","        return img, bboxes"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["\n","# Main training pipeline\n","def train_model():\n","\n","    # Load the MLflow configuration\n","    with open('mlflow_config.yaml', 'r') as file:\n","        mlflow_config = yaml.safe_load(file)\n","    \n","    # Start MLflow experiment\n","    mlflow.set_tracking_uri(mlflow_config['server_url'])\n","    mlflow.set_experiment(mlflow_config['experiment_name'])\n","\n","    with mlflow.start_run():\n","\n","        model = YoloV1(split_size=7, num_boxes=2, num_classes=3).to(DEVICE)\n","        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n","        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.1, patience=3, mode='max', verbose=True)\n","        loss_fn = YoloLoss()\n","\n","        # Log model parameters\n","        mlflow.log_param(\"split_size\", 7)\n","        mlflow.log_param(\"num_boxes\", 2)\n","        mlflow.log_param(\"num_classes\", 3)\n","        mlflow.log_param(\"learning_rate\", LEARNING_RATE)\n","        mlflow.log_param(\"weight_decay\", WEIGHT_DECAY)\n","        mlflow.log_param(\"batch_size\", BATCH_SIZE)\n","        mlflow.log_param(\"epochs\", EPOCHS)\n","\n","        # Load model checkpoint if required\n","        if LOAD_MODEL:\n","            load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n","\n","        transform = Compose([transforms.Resize((448, 448)), transforms.ToTensor()])\n","\n","        # Prepare data\n","        train_dataset = FruitImagesDataset(df=df, transform=transform, files_dir=files_dir)\n","        train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n","\n","        for epoch in range(EPOCHS):\n","            train_loss = train_one_epoch(train_loader, model, optimizer, loss_fn, epoch, EPOCHS)\n","\n","            mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n","\n","            train_mAP = evaluate_model(train_loader, model)\n","            print(f\"Train mAP for epoch {epoch + 1}: {train_mAP}\")\n","\n","            # Log mAP\n","            mlflow.log_metric(\"train_mAP\", train_mAP, step=epoch)\n","\n","            # Adjust learning rate\n","            scheduler.step(train_mAP)\n","\n","            # Save model checkpoint\n","            checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n","            save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE)\n","\n","            # Log the checkpoint as an artifact\n","            mlflow.log_artifact(LOAD_MODEL_FILE)\n","\n","        mlflow.pytorch.log_model(model, \"model\")\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024/10/12 22:27:23 INFO mlflow.tracking.fluent: Experiment with name 'YoloV1_from_scratch' does not exist. Creating a new experiment.\n","/opt/homebrew/Caskroom/miniconda/base/envs/YoloV1/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","/var/folders/xp/g653h5tx4tvdwlm5b5lcvkdm0000gn/T/ipykernel_20553/2020931513.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n"]},{"name":"stdout","output_type":"stream","text":["=> Loading Checkpoint\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [1/20]:  33%|███▎      | 5/15 [00:54<01:57, 11.77s/it, Batch=5/15, Loss=47.9679, Mean Loss=56.7138, LR=2e-5]/opt/homebrew/Caskroom/miniconda/base/envs/YoloV1/lib/python3.8/site-packages/PIL/Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","Epoch [1/20]: 100%|██████████| 15/15 [02:58<00:00, 11.90s/it, Batch=15/15, Loss=47.9110, Mean Loss=54.1809, LR=2e-5]\n"]},{"name":"stdout","output_type":"stream","text":["Mean loss for epoch 1: 54.1809\n","Train mAP for epoch 1: 0.7312399744987488\n","=> Saving Checkpoint\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [2/20]:  47%|████▋     | 7/15 [01:31<01:42, 12.78s/it, Batch=7/15, Loss=43.1191, Mean Loss=47.5606, LR=2e-5]"]}],"source":["\n","# Execute training\n","if __name__ == \"__main__\":\n","    train_model()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_h-9x5SAQefR"},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/xp/g653h5tx4tvdwlm5b5lcvkdm0000gn/T/ipykernel_20553/4041745455.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n"]},{"name":"stdout","output_type":"stream","text":["=> Loading Checkpoint\n"]},{"ename":"NameError","evalue":"name 'transform' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m     test_mAP \u001b[38;5;241m=\u001b[39m evaluate_model(test_loader, model)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest mAP: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_mAP\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mpredict_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[11], line 15\u001b[0m, in \u001b[0;36mpredict_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     load_checkpoint(torch\u001b[38;5;241m.\u001b[39mload(LOAD_MODEL_FILE), model, optimizer)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Prepare data\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m FruitImagesDataset(df\u001b[38;5;241m=\u001b[39mtest_df, transform\u001b[38;5;241m=\u001b[39m\u001b[43mtransform\u001b[49m, files_dir\u001b[38;5;241m=\u001b[39mtest_dir)\n\u001b[1;32m     16\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset\u001b[38;5;241m=\u001b[39mtest_dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m test_mAP \u001b[38;5;241m=\u001b[39m evaluate_model(test_loader, model)\n","\u001b[0;31mNameError\u001b[0m: name 'transform' is not defined"]}],"source":["LOAD_MODEL = True\n","LOAD_MODEL_FILE = \"model.pth\"\n","\n","# Prediction pipeline\n","def predict_model():\n","    model = YoloV1(split_size=7, num_boxes=2, num_classes=3).to(DEVICE)\n","    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n","    loss_fn = YoloLoss()\n","\n","    # Load the model\n","    if LOAD_MODEL:\n","        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n","\n","    transform = Compose([transforms.Resize((448, 448)), transforms.ToTensor()])\n","\n","    # Prepare data\n","    test_dataset = FruitImagesDataset(df=test_df, transform=transform, files_dir=test_dir)\n","    test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n","\n","    test_mAP = evaluate_model(test_loader, model)\n","    print(f\"Test mAP: {test_mAP}\")\n","\n","\n","predict_model()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.20"}},"nbformat":4,"nbformat_minor":0}
