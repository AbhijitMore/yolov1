{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TDz7YOvV-lKf","executionInfo":{"status":"ok","timestamp":1728708188960,"user_tz":-330,"elapsed":26161,"user":{"displayName":"Abhijit More","userId":"12603566416311184813"}},"outputId":"8351db00-2081-42b8-9872-219da0ce53b5"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"8YyeoM2Z-iuw"},"source":["# Import Libraries"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Vp9TvoXF-iu0","executionInfo":{"status":"ok","timestamp":1728708199959,"user_tz":-330,"elapsed":7641,"user":{"displayName":"Abhijit More","userId":"12603566416311184813"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import pandas as pd\n","import os\n","import xml.etree.ElementTree as ET\n","import PIL\n","from PIL import Image\n","from collections import Counter\n","from torchvision.transforms import transforms\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PwSCbURu-iu1","executionInfo":{"status":"ok","timestamp":1728708199959,"user_tz":-330,"elapsed":5,"user":{"displayName":"Abhijit More","userId":"12603566416311184813"}},"outputId":"489278df-4df8-416f-84b3-c2df957507c5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f54e812e310>"]},"metadata":{},"execution_count":3}],"source":["seed = 42\n","torch.manual_seed(seed)"]},{"cell_type":"markdown","metadata":{"id":"wrMPTJLA-iu2"},"source":["# Model Architecture"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"SJq4N0pK-iu2","executionInfo":{"status":"ok","timestamp":1728708204833,"user_tz":-330,"elapsed":3,"user":{"displayName":"Abhijit More","userId":"12603566416311184813"}}},"outputs":[],"source":["architecture_config = [\n","    #Tuple: (kernel_size, number_of_filters, strides, padding)\n","    # \"M\": Max Pool Layer\n","    (7, 64, 2, 3),\n","    \"M\",\n","    (3, 192, 1, 1),\n","    \"M\",\n","    (1, 128, 1, 1),\n","    (3, 256, 1, 1),\n","    (1, 256, 1, 0),\n","    (3, 512, 1, 1),\n","    \"M\",\n","    #List: [(tuple), (tuple), how many times to repeat]\n","    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n","    (1, 512, 1, 0),\n","    (3, 1024, 1, 1),\n","    \"M\",\n","    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n","    (3, 1024, 1, 1),\n","    (3, 1024, 2, 1),\n","    (3, 1024, 1, 1),\n","    (3, 1024, 1, 1),\n","    #Doesn't include FC layers\n","]"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"4Rw-15SN-iu3","executionInfo":{"status":"ok","timestamp":1728708204833,"user_tz":-330,"elapsed":2,"user":{"displayName":"Abhijit More","userId":"12603566416311184813"}}},"outputs":[],"source":["class CNNBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, **kwargs):\n","        super(CNNBlock, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n","        self.batchnorm = nn.BatchNorm2d(out_channels)\n","        self.leakyrelu = nn.LeakyReLU(0.1)\n","\n","    def forward(self, x):\n","        return self.leakyrelu(self.batchnorm(self.conv(x)))\n","\n","class YoloV1(nn.Module):\n","    def __init__(self, in_channels=3, **kwargs):\n","        super(YoloV1, self).__init__()\n","        self.architecture = architecture_config\n","        self.in_channels = in_channels\n","        self.darknet = self._create_conv_layers(self.architecture)\n","        self.fcs = self._create_fcs(**kwargs)\n","\n","    def forward(self, x):\n","        x = self.darknet(x)\n","        return self.fcs(torch.flatten(x, start_dim=1))\n","\n","    def _create_conv_layers(self, architecture):\n","        layers = []\n","        in_channels = self.in_channels\n","\n","        for x in architecture:\n","            if type(x) == tuple:\n","                layers+= [CNNBlock(in_channels, x[1], kernel_size=x[0], stride=x[2], padding=x[3])]\n","                in_channels = x[1]\n","            elif type(x) == str:\n","                layers+= [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            elif type(x) == list:\n","                conv1 = x[0] #tuple\n","                conv2 = x[1] #tuple\n","                repeats = x[2] #int\n","\n","                for _ in range(repeats):\n","                    layers+=[CNNBlock(in_channels, conv1[1], kernel_size=conv1[0], stride=conv1[2], padding=conv1[3])]\n","                    layers+=[CNNBlock(conv1[1], conv2[1], kernel_size=conv2[0], stride=conv2[2], padding=conv2[3])]\n","                    in_channels=conv2[1]\n","\n","        return nn.Sequential(*layers)\n","\n","    def _create_fcs(self, split_size, num_boxes, num_classes):\n","        S, B, C = split_size, num_boxes, num_classes\n","        return nn.Sequential(nn.Flatten(), nn.Linear(1024*S*S, 496), nn.Dropout(0.0), nn.LeakyReLU(0.1), nn.Linear(496, S*S*(C+B*5)))"]},{"cell_type":"markdown","metadata":{"id":"ea0aluWz-iu3"},"source":["# Utility Functions"]},{"cell_type":"markdown","metadata":{"id":"6fJ_U96s-iu4"},"source":["## Intersection over Union"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"glcYt5RE-iu4","executionInfo":{"status":"ok","timestamp":1728708475925,"user_tz":-330,"elapsed":620,"user":{"displayName":"Abhijit More","userId":"12603566416311184813"}}},"outputs":[],"source":["def intersection_over_union(boxes_preds, boxes_labels, box_format = 'midpoint'):\n","    \"\"\"\n","    Calculates Intersection over union\n","\n","    Parameters:\n","        boxes_preds (tensor): Prediction of Bounding boxes(BATCH_SIZE, 4)\n","        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)\n","        box_format (str): midpoint/corners, if boxes are (x,y,w,h) or (x1,y1,x2,y2) respectively\n","\n","    Returns:\n","        tensor: Intersection over union for all examples\n","    \"\"\"\n","    # boxes_preds shape is (N,4) where N is the number of predicted bboxes\n","    # boxes_labels shape is (n, 4)\n","\n","    if box_format == \"midpoint\":\n","        box1_x1 = boxes_preds[...,0:1] - boxes_preds[...,2:3] / 2\n","        box1_y1 = boxes_preds[...,1:2] - boxes_preds[...,3:4] / 2\n","        box1_x2 = boxes_preds[...,0:1] + boxes_preds[...,2:3] / 2\n","        box1_y2 = boxes_preds[...,1:2] + boxes_preds[...,3:4] / 2\n","\n","        box2_x1 = boxes_labels[...,0:1] - boxes_labels[...,2:3] / 2\n","        box2_y1 = boxes_labels[...,1:2] - boxes_labels[...,3:4] / 2\n","        box2_x2 = boxes_labels[...,0:1] + boxes_labels[...,2:3] / 2\n","        box2_y2 = boxes_labels[...,1:2] + boxes_labels[...,3:4] / 2\n","\n","    if box_format == \"corners\":\n","        box1_x1 = boxes_preds[...,0:1]\n","        box1_y1 = boxes_preds[...,1:2]\n","        box1_x2 = boxes_preds[...,2:3]\n","        box1_y2 = boxes_preds[...,3:4] #output tensor should (N,1). If we only use 3, we go to (N)\n","\n","        box2_x1 = boxes_labels[...,0:1]\n","        box2_y1 = boxes_labels[...,1:2]\n","        box2_x2 = boxes_labels[...,2:3]\n","        box2_y2 = boxes_labels[...,3:4]\n","\n","\n","    x1 = torch.max(box1_x1, box2_x1)\n","    y1 = torch.max(box1_y1, box2_y1)\n","    x2 = torch.min(box1_x2, box2_x2)\n","    y2 = torch.min(box1_y2, box2_y2)\n","\n","    # .clamp(0) is for the case when they don't intersect. Since when they don't intersect one of these will be negetive so they should become 0\n","    intersection = (x2 - x1).clamp(0) * (y2-y1).clamp(0)\n","\n","    box1_area = abs((box1_x2- box1_x1)*(box1_y2-box1_y1))\n","    box2_area = abs((box2_x2- box2_x1)*(box2_y2-box2_y1))\n","\n","    return intersection / (box1_area + box2_area - intersection + 1e-6)"]},{"cell_type":"markdown","metadata":{"id":"_g4tdqm5-iu4"},"source":["## Non Maximal Supression"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"tEeseaqr-iu5","executionInfo":{"status":"ok","timestamp":1728708478708,"user_tz":-330,"elapsed":590,"user":{"displayName":"Abhijit More","userId":"12603566416311184813"}}},"outputs":[],"source":["def non_max_supression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n","    \"\"\"\n","    Given Bounding Boxes does Non-maximal supression\n","\n","    Parameters:\n","        bboxes (list): list of lists containing all bounding boxes with each box specified as [class_preds, prob_score, x1, y2, x2, y2]\n","        iou_threshold: (float): threshold where predicted bounding box is correct\n","        threshold (float): threshold to remove predicted bboxes (Independent of IOU)\n","        box_format (str): midpoint/corners, if boxes are (x,y,w,h) or (x1,y1,x2,y2) respectively\n","\n","    Returns:\n","        bboxes_after_nms (list): bboxes after performing NMS given a specific IOU Threshold\n","    \"\"\"\n","\n","    assert type(bboxes) == list\n","\n","    bboxes = [box for box in bboxes if box[1]> threshold ]\n","    bboxes = sorted(bboxes, key= lambda x: x[1], reverse=True)\n","    bboxes_after_nms = []\n","\n","    while bboxes:\n","        chosen_box = bboxes.pop(0)\n","        bboxes = [box for box in bboxes\n","        if box[0]!=chosen_box[0] or intersection_over_union(torch.tensor(chosen_box[2:]), torch.tensor(box[2:]), box_format = box_format) < iou_threshold\n","        ]\n","        bboxes_after_nms.append(chosen_box)\n","\n","    return bboxes_after_nms"]},{"cell_type":"markdown","metadata":{"id":"ZqzjAEcR-iu5"},"source":["## Mean Average Precision"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"vzG_EzRv-iu5","executionInfo":{"status":"ok","timestamp":1728708479422,"user_tz":-330,"elapsed":2,"user":{"displayName":"Abhijit More","userId":"12603566416311184813"}}},"outputs":[],"source":["def mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20):\n","    \"\"\"\n","    Calculates Mean Average Precision\n","\n","    Parameters:\n","        pred_boxes (list): list of lists containing all bounding boxes with each box specified as [train_idx, class_preds, prob_score, x1, y1, x2, y2]\n","        true_boxes (list): similar to pred boxes except all the correct ones\n","        iou_threshold (float): threshold above which predicted box is correct\n","        box_format (str): midpoint/corners, if boxes are (x,y,w,h) or (x1,y1,x2,y2) respectively\n","        num_classes (int): number of classes\n","\n","    Returns:\n","        float: mAP value across all classes given a specific IOU threshold\n","\n","    \"\"\"\n","\n","    #list storing all AP for respective classes\n","    average_precisions = []\n","\n","    #used for numerical stability later on\n","    epsilon = 1e-6\n","\n","    for c in range(num_classes):\n","        detections = []\n","        ground_truths = []\n","\n","        # Go through all the predictions and targets and only add the ones that belong to the current class c\n","        for detection in pred_boxes:\n","            if detection[1]==c:\n","                detections.append(detection)\n","\n","        for true_box in true_boxes:\n","            if true_box[1]==c:\n","                ground_truths.append(true_box)\n","\n","        # find the amount of bboxes for each training example, Counter here finds how many ground truth boxes we get for each training example.\n","        # so lets say img 0 has 3 and img 1 has 5 then we will get dictionary with amount_bboxes={0:3, 1:5}\n","        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n","\n","        # We then go through each key, val in this dictionary and convert to the following (w.r.t. same example)\n","        #  amount_bboxes = {0: torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n","        for key, val in amount_bboxes.items():\n","            amount_bboxes[key] = torch.zeros(val)\n","\n","        # sort by box_probabilities which is index 2\n","        detections.sort(key= lambda x:x[2], reverse=True)\n","        TP = torch.zeros((len(detections)))\n","        FP = torch.zeros((len(detections)))\n","        total_true_bboxes = len(ground_truths)\n","\n","        # if none exists for this class then we can safely skip\n","        if total_true_bboxes == 0:\n","            continue\n","\n","        for detection_idx, detection in enumerate(detections):\n","            # only take out ground truths that have the same training_idx as detection\n","            ground_truth_img = [bbox for bbox in ground_truths if bbox[0] == detection[0]]\n","\n","            num_gts = len(ground_truth_img)\n","            best_iou = 0\n","\n","            for idx, gt in enumerate(ground_truth_img):\n","                iou = intersection_over_union(torch.tensor(detection[3:]), torch.tensor(gt[3:]), box_format= box_format)\n","                if iou > best_iou:\n","                    best_iou = iou\n","                    best_gt_idx = idx\n","\n","            if best_iou > iou_threshold:\n","                # only detect ground truth detection once\n","                if amount_bboxes[detection[0]][best_gt_idx]==0:\n","                    #true positive and add this bounding box to seen\n","                    TP[detection_idx] = 1\n","                    amount_bboxes[detection[0]][best_gt_idx] = 1\n","                else:\n","                    FP[detection_idx] = 1\n","            # if iou is lower then the detection is false positive\n","            else:\n","                FP[detection_idx] = 1\n","\n","        TP_cumsum = torch.cumsum(TP, dim=0)\n","        FP_cumsum = torch.cumsum(FP, dim=0)\n","        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n","        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n","        precisions = torch.cat((torch.tensor([1], dtype=precisions.dtype), precisions))\n","        recalls = torch.cat((torch.tensor([0], dtype=precisions.dtype), recalls))\n","        # torch.trapz for numerical integration\n","        average_precisions.append(torch.trapz(precisions, recalls))\n","\n","    return sum(average_precisions)/ len(average_precisions)\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"c4ZFoBdG-iu6","executionInfo":{"status":"ok","timestamp":1728708479422,"user_tz":-330,"elapsed":2,"user":{"displayName":"Abhijit More","userId":"12603566416311184813"}}},"outputs":[],"source":["def get_bboxes(loader, model, iou_threshold, threshold, pred_format = \"cells\", box_format = \"midpoint\", device= \"cuda\"):\n","\n","    all_pred_boxes = []\n","    all_true_boxes = []\n","\n","    # make sure model is in eval() mode before get bboxes\n","    model.eval()\n","    train_idx = 0\n","\n","    for batch_idx, (x, labels) in enumerate(loader):\n","        x = x.to(device)\n","        labels = labels.to(device)\n","\n","        with torch.no_grad():\n","            predictions = model(x)\n","\n","        batch_size = x.shape[0]\n","        true_bboxes = cellboxes_to_boxes(labels)\n","        bboxes = cellboxes_to_boxes(predictions)\n","\n","        for idx in range(batch_size):\n","            nms_boxes = non_max_supression(bboxes[idx],iou_threshold= iou_threshold, threshold = threshold, box_format = box_format)\n","\n","            for nms_box in nms_boxes:\n","                all_pred_boxes.append([train_idx]+ nms_box)\n","\n","            for box in true_bboxes[idx]:\n","                # many will get converted to 0 pred\n","                if box[1]>threshold:\n","                    all_true_boxes.append([train_idx]+box)\n","\n","            train_idx+=1\n","\n","    model.train()\n","    return all_pred_boxes, all_true_boxes\n","\n","def convert_cellboxes(predictions, S=7, C=3):\n","    \"\"\"\n","    Converts bounding boxes output from Yolo with an image split size of S into entire image ratios rather than relative to cell ratios.\n","    \"\"\"\n","\n","    predictions = predictions.to(\"cpu\")\n","    batch_size = predictions.shape[0]\n","    predictions = predictions.reshape(batch_size, 7, 7, C + 10)\n","    bboxes1 = predictions[..., C+1:C+5]\n","    bboxes2 = predictions[..., C+6: C+10]\n","    scores = torch.cat((predictions[...,C].unsqueeze(0), predictions[...,C+5].unsqueeze(0)), dim=0)\n","    best_box = scores.argmax(0).unsqueeze(-1)\n","    best_boxes = bboxes1 * (1-best_box)+ best_box * bboxes2\n","    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n","    x = 1 / S * (best_boxes[..., :1]+ cell_indices)\n","    y = 1 / S * (best_boxes[..., 1:2]+ cell_indices.permute(0,2,1,3))\n","    w_y = 1 / S * best_boxes[..., 2:4]\n","    converted_bboxes = torch.cat((x,y,w_y), dim=-1)\n","    predicted_class = predictions[...,:C].argmax(-1).unsqueeze(-1)\n","    best_confidence = torch.max(predictions[..., C], predictions[...,C+5]).unsqueeze(-1)\n","    converted_preds = torch.cat((predicted_class, best_confidence, converted_bboxes), dim=-1)\n","\n","    return converted_preds\n","\n","def cellboxes_to_boxes(out, S=7):\n","    converted_preds = convert_cellboxes(out).reshape(out.shape[0], S*S, -1)\n","    converted_preds[...,0] = converted_preds[..., 0].long()\n","    all_bboxes = []\n","\n","    for ex_idx in range(out.shape[0]):\n","        bboxes = []\n","\n","        for bbox_idx in range(S*S):\n","            bboxes.append([x.item() for x in converted_preds[ex_idx, bbox_idx, :]])\n","        all_bboxes.append(bboxes)\n","\n","    return all_bboxes\n","\n","def save_checkpoint(state, filename = \"my_checkpoint.pth\"):\n","    print(\"=> Saving Checkpoint\")\n","    torch.save(state, filename)\n","\n","def load_checkpoint(checkpoint, model, optimizer):\n","    print(\"=> Loading Checkpoint\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer\"])"]},{"cell_type":"markdown","metadata":{"id":"vQ0OPX9O-iu6"},"source":["# Dataset Preprocessing"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"mv_OgfqR-iu6","executionInfo":{"status":"ok","timestamp":1728708480038,"user_tz":-330,"elapsed":2,"user":{"displayName":"Abhijit More","userId":"12603566416311184813"}}},"outputs":[],"source":["files_dir = '/content/drive/MyDrive/yolov1/dataset/train_zip/train'\n","test_dir = '/content/drive/MyDrive/yolov1/dataset/test_zip/test'\n","\n","images = [image for image in sorted(os.listdir(files_dir)) if image[-4:]=='.jpg']\n","annots = [image[:-4] + '.xml' for image in images]\n","\n","images = pd.Series(images, name='images')\n","annots = pd.Series(annots, name='annots')\n","df = pd.concat([images, annots], axis=1)\n","df = pd.DataFrame(df)\n","\n","test_images = [image for image in sorted(os.listdir(test_dir)) if image[-4:]=='.jpg']\n","test_annots = [image[:-4] + '.xml' for image in test_images]\n","\n","test_images = pd.Series(test_images, name='test_images')\n","test_annots = pd.Series(test_annots, name='test_annots')\n","test_df = pd.concat([test_images, test_annots], axis=1)\n","test_df = pd.DataFrame(test_df)"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"KTtOgdoa-iu7","executionInfo":{"status":"ok","timestamp":1728711003296,"user_tz":-330,"elapsed":521,"user":{"displayName":"Abhijit More","userId":"12603566416311184813"}}},"outputs":[],"source":["class FruitImagesDataset(torch.utils.data.Dataset):\n","    def __init__(self, df=df, files_dir=files_dir, S=7, B=2, C=3, transform =None):\n","        self.annotations = df\n","        self.files_dir = files_dir\n","        self.transform = transform\n","        self.S = S\n","        self.B = B\n","        self.C = C\n","\n","    def __len__(self):\n","        return len(self.annotations)\n","\n","    def __getitem__(self, index):\n","        label_path = os.path.join(self.files_dir, self.annotations.iloc[index, 1])\n","        boxes = []\n","        tree = ET.parse(label_path)\n","        root = tree.getroot()\n","\n","        class_dictionary = {'apple': 0, 'banana': 1, 'orange':2}\n","\n","        if(int(root.find('size').find('height').text)==0):\n","            filename = root.find('filename').text\n","            img = Image.open(self.files_dir + '/' + filename)\n","            img_width, img_height = img.size\n","\n","            for member in root.findall('object'):\n","                klass = member.find('name').text\n","                klass = class_dictionary[klass]\n","\n","                #bounding box\n","                xmin = int(member.find('bndbox').find('xmin').text)\n","                xmax = int(member.find('bndbox').find('xmax').text)\n","                ymin = int(member.find('bndbox').find('ymin').text)\n","                ymax = int(member.find('bndbox').find('ymax').text)\n","\n","                centerx = ((xmax + xmin)/2) /img_width\n","                centery = ((ymax+ ymin)/2) / img_height\n","                boxwidth = (xmax - xmin) / img_width\n","                boxheight = (ymax - ymin) / img_height\n","\n","                boxes.append([klass, centerx, centery, boxwidth, boxheight])\n","\n","        elif(int(root.find('size').find('height').text)!=0):\n","\n","            for member in root.findall('object'):\n","                klass = member.find('name').text\n","                klass = class_dictionary[klass]\n","\n","                #bounding box\n","                xmin = int(member.find('bndbox').find('xmin').text)\n","                xmax = int(member.find('bndbox').find('xmax').text)\n","                img_width = int(root.find('size').find('width').text)\n","                ymin = int(member.find('bndbox').find('ymin').text)\n","                ymax = int(member.find('bndbox').find('ymax').text)\n","                img_height = int(root.find('size').find('height').text)\n","\n","                centerx = ((xmax + xmin)/2) /img_width\n","                centery = ((ymax+ ymin)/2) / img_height\n","                boxwidth = (xmax - xmin) / img_width\n","                boxheight = (ymax - ymin) / img_height\n","\n","                boxes.append([klass, centerx, centery, boxwidth, boxheight])\n","\n","        boxes = torch.tensor(boxes)\n","        img_path = os.path.join(self.files_dir, self.annotations.iloc[index, 0])\n","        image = Image.open(img_path)\n","        image = image.convert(\"RGB\")\n","\n","        if self.transform:\n","            image, boxes = self.transform(image, boxes)\n","\n","        #convert to cells\n","        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n","        for box in boxes:\n","            class_label, x, y, width, height = box.tolist()\n","            class_label = int(class_label)\n","\n","            #i,j represents the cell_row and cell_column\n","            i,j = int(self.S*y), int(self.S*x)\n","            x_cell, y_cell = self.S*x-j, self.S*y-i\n","\n","            \"\"\"\n","            Calculating the width and height of cell of bounding box, relative to the cell is done by following, with width as the example:\n","\n","            width_pixels = (width*self.image_width)\n","            cell_pixels = (self.image_width)\n","\n","            Then to find the width relative to the cell is simply:\n","            width_pixels/ cell_pixels, simplification leads to the formulas below.\n","            \"\"\"\n","\n","            width_cell, height_cell = (width * self.S, height * self.S)\n","\n","            #If no object already found for specific cell i, j Note: this means we restrict to ONE object per cell !\n","\n","            if label_matrix[i, j, self.C] == 0:\n","                #set that there exists an object\n","                label_matrix[i, j , self.C] = 1\n","                # box coordinates\n","                box_coordinates = torch.tensor([x_cell, y_cell, width_cell, height_cell])\n","\n","                label_matrix[i, j, 4:8] = box_coordinates\n","                label_matrix[i,j, class_label] = 1\n","\n","        return image, label_matrix"]},{"cell_type":"markdown","metadata":{"id":"y-RowFnV-iu7"},"source":["# Model Loss"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"1GnL4e6I-iu7","executionInfo":{"status":"ok","timestamp":1728711007304,"user_tz":-330,"elapsed":565,"user":{"displayName":"Abhijit More","userId":"12603566416311184813"}}},"outputs":[],"source":["class YoloLoss(nn.Module):\n","    \"\"\"\n","    Calculate the loss for YoloV1 model\n","    \"\"\"\n","    def __init__(self, S=7, B=2, C=3):\n","        super(YoloLoss, self).__init__()\n","        self.mse = nn.MSELoss(reduction=\"sum\")\n","\n","        \"\"\"\n","        S is split size of image (grid size) (in paper 7)\n","        B is number of boxes (in paper 2),\n","        C is number of classes (in paper 20, in dataset 3)\n","        \"\"\"\n","        self.S = S\n","        self.B = B\n","        self.C = C\n","\n","        self.lambda_noobj = 0.5\n","        self.lambda_coord = 5\n","\n","    def forward(self, predictions, target):\n","        # input prediction shape: (BATCH_SIZE, S*S*(C+B*5))\n","        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n","\n","        #Calculate IOU for two predicted bounding boxes with target box\n","        iou_b1 = intersection_over_union(predictions[...,self.C + 1: self.C+5], target[..., self.C + 1: self.C + 5])\n","        iou_b2 = intersection_over_union(predictions[...,self.C + 6: self.C+10], target[..., self.C + 1: self.C + 5])\n","        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n","\n","        #Take the box with highest IOU out of the two predictions. Note: bestbox will be indices 0,1 for which bbox was best\n","        iou_maxes, bestbox = torch.max(ious, dim=0)\n","        exists_box = target[..., self.C].unsqueeze(3)\n","\n","        # =========================#\n","        # FOR BOX COORDINATES      #\n","        # =========================#\n","\n","        #set boxes with no object in them to 0. We only take out one of the two predictions, which is one with highest IOUs\n","        box_predictions = exists_box * ((bestbox * predictions[..., self.C + 6: self.C+10]+ (1- bestbox)* predictions[..., self.C + 1: self.C + 5]))\n","        box_targets = exists_box * target[..., self.C + 1: self.C+ 5]\n","\n","        # take sqrt of width and height of box\n","        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4])* torch.sqrt(torch.abs(box_predictions[...,2:4]+ 1e-6))\n","        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n","\n","        box_loss = self.mse(torch.flatten(box_predictions, end_dim=-2),\n","                            torch.flatten(box_targets, end_dim=-2))\n","\n","        # =========================#\n","        # FOR  OBJECT LOSS      #\n","        # =========================#\n","\n","        #pred_box is the confidence score for bbox with highest IOU\n","        pred_box = (bestbox * predictions[..., self.C + 5:self.C+6] + (1-bestbox)* predictions[..., self.C:self.C+1])\n","        object_loss = self.mse(torch.flatten(exists_box* pred_box),\n","                                torch.flatten(exists_box * target[..., self.C: self.C+1]))\n","\n","\n","        # =========================#\n","        # FOR  No OBJECT LOSS      #\n","        # =========================#\n","\n","        no_object_loss = self.mse(torch.flatten((1-exists_box)* predictions[..., self.C:self.C+1], start_dim=1),\n","                                torch.flatten((1-exists_box)* target[..., self.C:self.C+1], start_dim=1))\n","\n","        no_object_loss += self.mse(torch.flatten((1-exists_box)* predictions[..., self.C+5:self.C+6], start_dim=1),\n","                                torch.flatten((1-exists_box)* target[..., self.C:self.C+1], start_dim=1))\n","\n","        # =========================#\n","        # FOR  CLASS LOSS      #\n","        # =========================#\n","\n","        class_loss = self.mse(torch.flatten(exists_box * predictions[..., :self.C], end_dim=-2),\n","                            torch.flatten(exists_box * target[..., :self.C], end_dim=-2))\n","\n","        loss = (self.lambda_coord * box_loss + object_loss + self.lambda_noobj * no_object_loss + class_loss)\n","\n","        return loss\n"]},{"cell_type":"markdown","metadata":{"id":"-wVqK_Ye-iu8"},"source":["# Model Training"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"KYqEjpii-iu8","executionInfo":{"status":"ok","timestamp":1728711008009,"user_tz":-330,"elapsed":1,"user":{"displayName":"Abhijit More","userId":"12603566416311184813"}}},"outputs":[],"source":["LEARNING_RATE = 2e-5\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","BATCH_SIZE = 16\n","WEIGHT_DECAY = 0\n","EPOCHS = 20\n","NUM_WORKERS = 2\n","PIN_MEMORY = True\n","LOAD_MODEL = False\n","LOAD_MODEL_FILE = \"model.pth\""]},{"cell_type":"code","execution_count":44,"metadata":{"id":"zroDk2Og-iu8","executionInfo":{"status":"ok","timestamp":1728711008010,"user_tz":-330,"elapsed":2,"user":{"displayName":"Abhijit More","userId":"12603566416311184813"}}},"outputs":[],"source":["def train_fn(train_loader, model, optimizer, loss_fn):\n","\n","    loop = tqdm(train_loader, leave=True)\n","    mean_loss = []\n","\n","    for batch_idx, (x,y) in enumerate(loop):\n","        x,y = x.to(DEVICE), y.to(DEVICE)\n","        out = model(x)\n","        loss = loss_fn(out, y)\n","        mean_loss.append(loss.item())\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        loop.set_postfix(loss= loss.item())\n","\n","    print(f\"Mean loss was: {sum(mean_loss)/ len(mean_loss)}\")"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"H_rgCCX8-iu8","executionInfo":{"status":"ok","timestamp":1728711008717,"user_tz":-330,"elapsed":3,"user":{"displayName":"Abhijit More","userId":"12603566416311184813"}}},"outputs":[],"source":["class Compose(object):\n","    def __init__(self,transforms):\n","        self.transforms = transforms\n","\n","    def __call__(self, img, bboxes):\n","        for t in self.transforms:\n","            img, bboxes = t(img), bboxes\n","\n","        return img, bboxes\n","\n","transform = Compose([transforms.Resize((448, 448)), transforms.ToTensor()])"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5cAIlRKs-iu9","executionInfo":{"status":"ok","timestamp":1728711507777,"user_tz":-330,"elapsed":499062,"user":{"displayName":"Abhijit More","userId":"12603566416311184813"}},"outputId":"ffceee95-ed4f-4cde-b881-c0d325f35c31"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n"," 60%|██████    | 9/15 [00:06<00:04,  1.32it/s, loss=542]/usr/local/lib/python3.10/dist-packages/PIL/Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","100%|██████████| 15/15 [00:10<00:00,  1.37it/s, loss=443]\n"]},{"output_type":"stream","name":"stdout","text":["Mean loss was: 657.2576904296875\n","Train mAP: 0.0\n","=> Saving Checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15/15 [00:11<00:00,  1.28it/s, loss=318]\n"]},{"output_type":"stream","name":"stdout","text":["Mean loss was: 328.755815633138\n","Train mAP: 0.0006210627034306526\n","=> Saving Checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15/15 [00:10<00:00,  1.41it/s, loss=190]\n"]},{"output_type":"stream","name":"stdout","text":["Mean loss was: 208.01370951334636\n","Train mAP: 0.025518914684653282\n","=> Saving Checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15/15 [00:10<00:00,  1.37it/s, loss=145]\n"]},{"output_type":"stream","name":"stdout","text":["Mean loss was: 153.5544896443685\n","Train mAP: 0.05154731869697571\n","=> Saving Checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15/15 [00:11<00:00,  1.28it/s, loss=108]\n"]},{"output_type":"stream","name":"stdout","text":["Mean loss was: 122.17223358154297\n","Train mAP: 0.11875223368406296\n","=> Saving Checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15/15 [00:11<00:00,  1.30it/s, loss=125]\n"]},{"output_type":"stream","name":"stdout","text":["Mean loss was: 100.62308756510417\n","Train mAP: 0.1830100417137146\n","=> Saving Checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15/15 [00:10<00:00,  1.37it/s, loss=69.6]\n"]},{"output_type":"stream","name":"stdout","text":["Mean loss was: 86.49115651448568\n","Train mAP: 0.24958324432373047\n","=> Saving Checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15/15 [00:10<00:00,  1.37it/s, loss=72.2]\n"]},{"output_type":"stream","name":"stdout","text":["Mean loss was: 79.61641794840494\n","Train mAP: 0.3109549880027771\n","=> Saving Checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15/15 [00:11<00:00,  1.35it/s, loss=71.1]\n"]},{"output_type":"stream","name":"stdout","text":["Mean loss was: 72.28537979125977\n","Train mAP: 0.39996519684791565\n","=> Saving Checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15/15 [00:11<00:00,  1.32it/s, loss=52.9]\n"]},{"output_type":"stream","name":"stdout","text":["Mean loss was: 67.65903879801432\n","Train mAP: 0.464704304933548\n","=> Saving Checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15/15 [00:10<00:00,  1.39it/s, loss=65.1]\n"]},{"output_type":"stream","name":"stdout","text":["Mean loss was: 62.03799819946289\n","Train mAP: 0.5532524585723877\n","=> Saving Checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15/15 [00:11<00:00,  1.32it/s, loss=73]\n"]},{"output_type":"stream","name":"stdout","text":["Mean loss was: 57.28581008911133\n","Train mAP: 0.6472733020782471\n","=> Saving Checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15/15 [00:10<00:00,  1.39it/s, loss=49.4]\n"]},{"output_type":"stream","name":"stdout","text":["Mean loss was: 53.12489700317383\n","Train mAP: 0.6459515690803528\n","=> Saving Checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15/15 [00:10<00:00,  1.41it/s, loss=44]\n"]},{"output_type":"stream","name":"stdout","text":["Mean loss was: 53.271166483561196\n","Train mAP: 0.7243680953979492\n","=> Saving Checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15/15 [00:11<00:00,  1.29it/s, loss=42.4]\n"]},{"output_type":"stream","name":"stdout","text":["Mean loss was: 51.50519638061523\n","Train mAP: 0.7808558940887451\n","=> Saving Checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15/15 [00:11<00:00,  1.28it/s, loss=43.4]\n"]},{"output_type":"stream","name":"stdout","text":["Mean loss was: 49.926316324869795\n","Train mAP: 0.8021078109741211\n","=> Saving Checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15/15 [00:10<00:00,  1.42it/s, loss=52]\n"]},{"output_type":"stream","name":"stdout","text":["Mean loss was: 45.19901809692383\n","Train mAP: 0.8243246078491211\n","=> Saving Checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15/15 [00:11<00:00,  1.36it/s, loss=48.4]\n"]},{"output_type":"stream","name":"stdout","text":["Mean loss was: 42.95477981567383\n","Train mAP: 0.8568220138549805\n","=> Saving Checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15/15 [00:11<00:00,  1.31it/s, loss=47.7]\n"]},{"output_type":"stream","name":"stdout","text":["Mean loss was: 41.886949412027995\n","Train mAP: 0.8474331498146057\n","=> Saving Checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15/15 [00:11<00:00,  1.32it/s, loss=33.8]\n"]},{"output_type":"stream","name":"stdout","text":["Mean loss was: 39.793511962890626\n","Train mAP: 0.8804007172584534\n","=> Saving Checkpoint\n"]}],"source":["def main():\n","\n","    model = YoloV1(split_size= 7, num_boxes= 2, num_classes= 3).to(DEVICE)\n","    optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer = optimizer, factor = 0.1, patience = 3, mode='max', verbose=True)\n","    loss_fn = YoloLoss()\n","\n","    if LOAD_MODEL:\n","        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n","\n","    train_dataset = FruitImagesDataset(df=df, transform = transform,files_dir = files_dir)\n","    test_dataset = FruitImagesDataset(df=test_df, transform = transform,files_dir = test_dir)\n","\n","    train_loader = DataLoader(dataset = train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n","    test_loader = DataLoader(dataset = test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n","\n","    for epoch in range(EPOCHS):\n","        train_fn(train_loader, model, optimizer, loss_fn)\n","        pred_boxes, target_boxes = get_bboxes(train_loader, model, iou_threshold=0.5, threshold=0.4, device=DEVICE)\n","        mean_avg_prec = mean_average_precision(pred_boxes, target_boxes, iou_threshold=0.5, box_format = 'midpoint', num_classes=3)\n","\n","        print(f\"Train mAP: {mean_avg_prec}\")\n","        scheduler.step(mean_avg_prec)\n","\n","        checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n","        save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE)\n","\n","main()\n"]},{"cell_type":"markdown","metadata":{"id":"ONrcKQsk-iu9"},"source":["# Predictions"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"H3-vwRdv-iu9","executionInfo":{"status":"ok","timestamp":1728711538604,"user_tz":-330,"elapsed":582,"user":{"displayName":"Abhijit More","userId":"12603566416311184813"}}},"outputs":[],"source":["LOAD_MODEL = True\n","EPOCHS = 1"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W0wudELl-iu9","executionInfo":{"status":"ok","timestamp":1728711559324,"user_tz":-330,"elapsed":6760,"user":{"displayName":"Abhijit More","userId":"12603566416311184813"}},"outputId":"8f9ae047-823c-4b74-aeb0-a9850bcda191"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-51-542b63aa8ec4>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n"]},{"output_type":"stream","name":"stdout","text":["=> Loading Checkpoint\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:02<00:00,  1.49it/s, loss=132]\n"]},{"output_type":"stream","name":"stdout","text":["Mean loss was: 131.1020278930664\n","Test mAP: 0.17903821170330048\n"]}],"source":["def predictions():\n","\n","    model = YoloV1(split_size = 7, num_boxes= 2, num_classes=3).to(DEVICE)\n","    optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n","    loss_fn = YoloLoss()\n","\n","    if LOAD_MODEL:\n","        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n","\n","    test_dataset = FruitImagesDataset(df= test_df, transform = transform,files_dir = test_dir)\n","    test_loader = DataLoader(dataset = test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n","\n","    for epoch in range(EPOCHS):\n","        model.eval()\n","        train_fn(test_loader, model, optimizer, loss_fn)\n","        pred_boxes, target_boxes = get_bboxes(test_loader, model, iou_threshold=0.5, threshold=0.4, device=DEVICE)\n","        mean_avg_prec = mean_average_precision(pred_boxes, target_boxes, iou_threshold= 0.5, box_format=\"midpoint\", num_classes=3)\n","\n","        print(f\"Test mAP: {mean_avg_prec}\")\n","\n","predictions()"]},{"cell_type":"code","execution_count":59,"metadata":{"id":"WaG4QFH4-iu9","executionInfo":{"status":"ok","timestamp":1728712528349,"user_tz":-330,"elapsed":515,"user":{"displayName":"Abhijit More","userId":"12603566416311184813"}}},"outputs":[],"source":[]},{"cell_type":"code","source":[],"metadata":{"id":"_h-9x5SAQefR"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.20"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}